{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e39212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation, colors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing as sk_preprocessing\n",
    "from sklearn.metrics import accuracy_score #scoring\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy.ma.core import ceil\n",
    "from scipy.spatial import distance #distance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e87e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pressure = '/Users/votri/Downloads/DSDA 385 Machine Learning/Final Project - Climate Change/Pressure/MinMaxNormWithLabel/'\n",
    "\n",
    "wind = '/Users/votri/Downloads/DSDA 385 Machine Learning/Final Project - Climate Change/CESMU200/MinMaxNormWithLabel/'\n",
    "\n",
    "# Get the list of subfolders\n",
    "subfolders = [subfolder for subfolder in os.listdir(pressure) if os.path.isdir(os.path.join(pressure, subfolder))]\n",
    "\n",
    "dfs_pressure = []\n",
    "dfs_wind = []\n",
    "reshaped_arrays = []\n",
    "folder_names = []\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    pressure_path = os.path.join(pressure, subfolder)\n",
    "    wind_path = os.path.join(wind, subfolder)\n",
    "    files = [f for f in os.listdir(pressure_path) if f.endswith('.csv')]\n",
    "    for file in files:\n",
    "        # pressure data\n",
    "        pressure_file_path = os.path.join(pressure_path, file)\n",
    "        df_pressure = pd.read_csv(pressure_file_path, header=None)\n",
    "        df_pressure = np.array(df_pressure)\n",
    "        dfs_pressure.append(df_pressure)\n",
    "        \n",
    "        # wind data\n",
    "        wind_file_path = os.path.join(wind_path, file)\n",
    "        df_wind = pd.read_csv(wind_file_path, header=None)\n",
    "        df_wind = np.array(df_wind)\n",
    "        dfs_wind.append(df_wind)\n",
    "        \n",
    "        # stack 2 arrays\n",
    "        df = np.dstack((df_pressure, df_wind))\n",
    "        (h,w,c) = df.shape\n",
    "        df_2D = df.reshape(h*w,c) # reshape df to 2D\n",
    "        \n",
    "        reshaped_array = np.reshape(df, (1,-1)) # Reshape the dataframe into a 1D array with 225 attributes\n",
    "        reshaped_arrays.append(reshaped_array)  # Append the reshaped array to the list\n",
    "        \n",
    "        folder_names.append(subfolder) # Save the class of the image\n",
    "        \n",
    "# Concatenate the reshaped arrays into a single array\n",
    "concatenated_array = np.concatenate(reshaped_arrays, axis=0)\n",
    "\n",
    "# Convert the concatenated array into a dataframe\n",
    "result_df = pd.DataFrame(concatenated_array)\n",
    "result_df['Class'] = folder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eb99cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6  \\\n",
      "0    0.139692  0.000000  0.139077  0.000000  0.137846  0.000000  0.137231   \n",
      "1    0.475692  0.222222  0.473846  0.218182  0.471385  0.196429  0.468923   \n",
      "2    0.248615  0.106383  0.243692  0.083333  0.240000  0.062500  0.237538   \n",
      "3    0.248615  0.106383  0.243692  0.083333  0.240000  0.062500  0.237538   \n",
      "4    0.293538  0.208333  0.284923  0.163265  0.278154  0.120000  0.272615   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "732  0.844923  0.400000  0.857231  0.312500  0.872615  0.272727  0.886154   \n",
      "733  0.912615  0.205882  0.915692  0.228571  0.923692  0.184211  0.929846   \n",
      "734  0.931077  0.305556  0.933538  0.315068  0.931692  0.328767  0.931692   \n",
      "735  0.868923  0.603774  0.875077  0.509091  0.880000  0.413793  0.883692   \n",
      "736  0.856000  0.028986  0.864000  0.014706  0.874462  0.058824  0.876923   \n",
      "\n",
      "            7         8         9  ...       441       442       443  \\\n",
      "0    0.000000  0.137231  0.023256  ...  0.388889  0.082462  0.381818   \n",
      "1    0.157895  0.465846  0.135593  ...  0.500000  0.425846  0.483871   \n",
      "2    0.041667  0.235692  0.040816  ...  0.537037  0.222769  0.555556   \n",
      "3    0.041667  0.235692  0.040816  ...  0.537037  0.222769  0.555556   \n",
      "4    0.061224  0.268923  0.020408  ...  0.421053  0.221538  0.421053   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "732  0.264706  0.890462  0.277778  ...  0.367347  0.889231  0.384615   \n",
      "733  0.119048  0.932308  0.111111  ...  0.516129  0.926154  0.444444   \n",
      "734  0.328767  0.931077  0.333333  ...  0.408451  0.921846  0.352113   \n",
      "735  0.366667  0.884923  0.317460  ...  0.455882  0.892923  0.477612   \n",
      "736  0.119403  0.873846  0.164179  ...  0.210526  0.935385  0.214286   \n",
      "\n",
      "          444       445       446       447       448       449  Class  \n",
      "0    0.086154  0.400000  0.092308  0.415094  0.102769  0.431373     CL  \n",
      "1    0.429538  0.467742  0.435077  0.459016  0.443077  0.423729     CL  \n",
      "2    0.232615  0.555556  0.246154  0.566038  0.264615  0.588235     CL  \n",
      "3    0.232615  0.555556  0.246154  0.566038  0.264615  0.588235     CL  \n",
      "4    0.229538  0.421053  0.240000  0.438596  0.253538  0.454545     CL  \n",
      "..        ...       ...       ...       ...       ...       ...    ...  \n",
      "732  0.896000  0.370370  0.899077  0.357143  0.900308  0.383333   NROI  \n",
      "733  0.926769  0.375000  0.927385  0.328125  0.928615  0.265625   NROI  \n",
      "734  0.923077  0.323944  0.920615  0.291667  0.921846  0.250000   NROI  \n",
      "735  0.898462  0.507463  0.904000  0.500000  0.913846  0.476190   NROI  \n",
      "736  0.933538  0.236364  0.931077  0.254545  0.927385  0.240741   NROI  \n",
      "\n",
      "[737 rows x 451 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "      <td>737.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.653576</td>\n",
       "      <td>0.304457</td>\n",
       "      <td>0.652917</td>\n",
       "      <td>0.293300</td>\n",
       "      <td>0.652001</td>\n",
       "      <td>0.282113</td>\n",
       "      <td>0.650932</td>\n",
       "      <td>0.271847</td>\n",
       "      <td>0.649845</td>\n",
       "      <td>0.263561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643169</td>\n",
       "      <td>0.476756</td>\n",
       "      <td>0.647061</td>\n",
       "      <td>0.472584</td>\n",
       "      <td>0.652169</td>\n",
       "      <td>0.468764</td>\n",
       "      <td>0.658171</td>\n",
       "      <td>0.465328</td>\n",
       "      <td>0.664773</td>\n",
       "      <td>0.463245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.259836</td>\n",
       "      <td>0.283046</td>\n",
       "      <td>0.261524</td>\n",
       "      <td>0.281538</td>\n",
       "      <td>0.262980</td>\n",
       "      <td>0.278370</td>\n",
       "      <td>0.264169</td>\n",
       "      <td>0.276129</td>\n",
       "      <td>0.265151</td>\n",
       "      <td>0.274591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279557</td>\n",
       "      <td>0.277604</td>\n",
       "      <td>0.279201</td>\n",
       "      <td>0.276394</td>\n",
       "      <td>0.278348</td>\n",
       "      <td>0.275974</td>\n",
       "      <td>0.277009</td>\n",
       "      <td>0.276022</td>\n",
       "      <td>0.275190</td>\n",
       "      <td>0.277787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.483692</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.478769</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.473846</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.468308</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447385</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.451077</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.459692</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.467692</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.478154</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.719385</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.721231</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.721231</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.716923</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>0.710769</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709538</td>\n",
       "      <td>0.518072</td>\n",
       "      <td>0.713231</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.719385</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.726154</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>0.472727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.882462</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.888615</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.894769</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.899692</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.901538</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903385</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.910154</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>0.916308</td>\n",
       "      <td>0.681319</td>\n",
       "      <td>0.918769</td>\n",
       "      <td>0.674157</td>\n",
       "      <td>0.922462</td>\n",
       "      <td>0.673077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987692</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 450 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5    \\\n",
       "count  737.000000  737.000000  737.000000  737.000000  737.000000  737.000000   \n",
       "mean     0.653576    0.304457    0.652917    0.293300    0.652001    0.282113   \n",
       "std      0.259836    0.283046    0.261524    0.281538    0.262980    0.278370   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.483692    0.062500    0.480000    0.056604    0.478769    0.053333   \n",
       "50%      0.719385    0.211538    0.721231    0.188679    0.721231    0.181818   \n",
       "75%      0.882462    0.514286    0.888615    0.490566    0.894769    0.486486   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "              6           7           8           9    ...         440  \\\n",
       "count  737.000000  737.000000  737.000000  737.000000  ...  737.000000   \n",
       "mean     0.650932    0.271847    0.649845    0.263561  ...    0.643169   \n",
       "std      0.264169    0.276129    0.265151    0.274591  ...    0.279557   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      0.473846    0.051282    0.468308    0.041667  ...    0.447385   \n",
       "50%      0.716923    0.163636    0.710769    0.156250  ...    0.709538   \n",
       "75%      0.899692    0.437500    0.901538    0.424242  ...    0.903385   \n",
       "max      1.000000    1.000000    1.000000    1.000000  ...    0.995077   \n",
       "\n",
       "              441         442         443         444         445         446  \\\n",
       "count  737.000000  737.000000  737.000000  737.000000  737.000000  737.000000   \n",
       "mean     0.476756    0.647061    0.472584    0.652169    0.468764    0.658171   \n",
       "std      0.277604    0.279201    0.276394    0.278348    0.275974    0.277009   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.265306    0.451077    0.254902    0.459692    0.254902    0.467692   \n",
       "50%      0.518072    0.713231    0.500000    0.719385    0.500000    0.726154   \n",
       "75%      0.687500    0.910154    0.690141    0.916308    0.681319    0.918769   \n",
       "max      1.000000    0.992000    1.000000    0.988308    1.000000    0.986462   \n",
       "\n",
       "              447         448         449  \n",
       "count  737.000000  737.000000  737.000000  \n",
       "mean     0.465328    0.664773    0.463245  \n",
       "std      0.276022    0.275190    0.277787  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.256410    0.478154    0.250000  \n",
       "50%      0.488372    0.736000    0.472727  \n",
       "75%      0.674157    0.922462    0.673077  \n",
       "max      1.000000    0.987692    1.000000  \n",
       "\n",
       "[8 rows x 450 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(result_df)\n",
    "result_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac4950f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COL     327\n",
      "CL      178\n",
      "COH     161\n",
      "NROI     71\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counts = result_df['Class'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f7f8e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "732    1\n",
      "733    1\n",
      "734    1\n",
      "735    1\n",
      "736    1\n",
      "Name: Class, Length: 737, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = result_df.drop('Class', axis = 1)\n",
    "Y = result_df['Class']\n",
    "\n",
    "Y_numerized = Y.replace({'CL': 0, 'COL' : 2, 'COH': 3, 'NROI': 1})\n",
    "print(Y_numerized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94bc16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# Data Normalization\n",
    "def minmax_scaler(data):\n",
    "  scaler = MinMaxScaler()\n",
    "  scaled = scaler.fit_transform(data)\n",
    "  return scaled\n",
    "\n",
    "# Euclidean distance\n",
    "def e_distance(x,y):\n",
    "  return distance.euclidean(x,y)\n",
    "\n",
    "# Manhattan distance\n",
    "def m_distance(x,y):\n",
    "  return distance.cityblock(x,y)\n",
    "\n",
    "# Best Matching Unit search\n",
    "def winning_neuron(data, t, som, num_rows, num_cols):\n",
    "  winner = [0,0]\n",
    "  shortest_distance = np.sqrt(data.shape[1]) # initialise with max distance\n",
    "  input_data = data[t]\n",
    "  for row in range(num_rows):\n",
    "    for col in range(num_cols):\n",
    "      distance = e_distance(som[row][col], data[t])\n",
    "      if distance < shortest_distance: \n",
    "        shortest_distance = distance\n",
    "        winner = [row,col]\n",
    "  return winner\n",
    "\n",
    "# Learning rate and neighbourhood range calculation\n",
    "def decay(step, max_steps,max_learning_rate,max_m_dsitance):\n",
    "  coefficient = 1.0 - (np.float64(step)/max_steps)\n",
    "  learning_rate = coefficient*max_learning_rate\n",
    "  neighbourhood_range = ceil(coefficient * max_m_dsitance)\n",
    "  return learning_rate, neighbourhood_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c69a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mian function\n",
    "\n",
    "train_x_norm = minmax_scaler(X) \n",
    "data = minmax_scaler(X) \n",
    "# initialising self-organising map\n",
    "num_dims = train_x_norm.shape[1] # numnber of dimensions in the input data\n",
    "np.random.seed(40)\n",
    "\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c569687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters  \n",
    "grid = [[15, 15]]\n",
    "max_m_dsitance = 4\n",
    "max_learning_rate = 0.5\n",
    "max_steps = int(150001)\n",
    "loop = [500, 1000, 5000, 10000, 20000, 50000, 75000, 100000, 125000, 150000]\n",
    "\n",
    "# num_nurons = 5*np.sqrt(train_x.shape[0])\n",
    "# grid_size = ceil(np.sqrt(num_nurons))\n",
    "# print(grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc29f88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 15 x 15\n",
      "- Accuracy at iteration 500: 0.858887381275441\n",
      "- Accuracy at iteration 1000: 0.8548168249660787\n",
      "- Accuracy at iteration 5000: 0.8575305291723202\n",
      "- Accuracy at iteration 10000: 0.8548168249660787\n",
      "- Accuracy at iteration 20000: 0.8697421981004071\n",
      "- Accuracy at iteration 50000: 0.8873812754409769\n",
      "- Accuracy at iteration 75000: 0.8778833107191316\n",
      "- Accuracy at iteration 100000: 0.8833107191316146\n",
      "- Accuracy at iteration 125000: 0.8968792401628223\n",
      "- Accuracy at iteration 150000: 0.8941655359565808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(grid)):\n",
    "  num_rows = grid[j][0]\n",
    "  num_cols = grid[j][1]\n",
    "  som = np.random.random_sample(size=(num_rows, num_cols, num_dims)) # map construction\n",
    "  print(f\"Grid size: {num_rows} x {num_cols}\")\n",
    "    \n",
    "  for step in range(max_steps):\n",
    "    learning_rate, neighbourhood_range = decay(step, max_steps,max_learning_rate,max_m_dsitance)\n",
    "    t = np.random.randint(0,high=train_x_norm.shape[0]) # random index of traing data\n",
    "    winner = winning_neuron(train_x_norm, t, som, num_rows, num_cols)\n",
    "    \n",
    "    for row in range(num_rows):\n",
    "      for col in range(num_cols):\n",
    "        if m_distance([row,col],winner) <= neighbourhood_range:\n",
    "          som[row][col] += learning_rate*(train_x_norm[t]-som[row][col]) #update neighbour's weight\n",
    "        \n",
    "    for i in loop:\n",
    "      if step == i:\n",
    "        label_data = np.array(Y_numerized)\n",
    "        map = np.empty(shape=(num_rows, num_cols), dtype=object)\n",
    "\n",
    "        for row in range(num_rows):\n",
    "          for col in range(num_cols):\n",
    "            map[row][col] = [] # empty list to store the label\n",
    "\n",
    "        for t in range(train_x_norm.shape[0]):\n",
    "          winner = winning_neuron(train_x_norm, t, som, num_rows, num_cols)\n",
    "          map[winner[0]][winner[1]].append(label_data[t]) # label of winning neuron\n",
    "        \n",
    "        label_map = np.zeros(shape=(num_rows, num_cols),dtype=np.int64)\n",
    "        for row in range(num_rows):\n",
    "          for col in range(num_cols):\n",
    "            label_list = map[row][col]\n",
    "            if len(label_list)==0:\n",
    "              label = 2\n",
    "            else:\n",
    "              label = max(label_list, key=label_list.count)\n",
    "            label_map[row][col] = label\n",
    "        \n",
    "        winner_labels = []\n",
    "\n",
    "        for t in range(data.shape[0]):\n",
    "          winner = winning_neuron(data, t, som, num_rows, num_cols)\n",
    "          row = winner[0]\n",
    "          col = winner[1]\n",
    "          predicted = label_map[row][col]\n",
    "          winner_labels.append(predicted)\n",
    "        \n",
    "        accuracy = accuracy_score(Y_numerized, np.array(winner_labels))\n",
    "        print(f\"- Accuracy at iteration {i}:\", accuracy)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf8d28d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy for SOM is 89.69%\n",
      "The best grid size is [15, 15] at iteration 125000\n"
     ]
    }
   ],
   "source": [
    "#print(accuracies)\n",
    "best_accuracy_som = max(accuracies)\n",
    "print(f'The best accuracy for SOM is {round(best_accuracy_som*100, 2)}%')\n",
    "index = accuracies.index(best_accuracy_som)\n",
    "grid_size = grid[math.floor(index/10)]\n",
    "best_iteration = loop[index%10]\n",
    "print(f'The best grid size is {grid_size} at iteration {best_iteration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fff3443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[168   0  10   0]\n",
      " [  0  48  18   5]\n",
      " [ 35   7 284   1]\n",
      " [  1   0   1 159]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(Y_numerized, np.array(winner_labels))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "173847e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# construct label map\n",
    "label_map = np.zeros(shape=(num_rows, num_cols),dtype=np.int64)\n",
    "for row in range(num_rows):\n",
    "  for col in range(num_cols):\n",
    "    label_list = map[row][col]\n",
    "    if len(label_list)==0:\n",
    "      label = 2\n",
    "    else:\n",
    "      label = max(label_list, key=label_list.count)\n",
    "    label_map[row][col] = label\n",
    "\n",
    "title = ('Iteration ' + str(max_steps))\n",
    "cmap = colors.ListedColormap(['tab:green', 'tab:red', 'tab:orange', 'tab:purple'])\n",
    "plt.imshow(label_map, cmap=cmap)\n",
    "plt.colorbar()\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
